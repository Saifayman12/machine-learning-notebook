# -*- coding: utf-8 -*-
"""ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-lHs0JfhpvutMfjvnl9MoHFHmzAuKqg7
"""

import numpy as np
# numpy is a library for handeling large number of data
import pandas as pd
# to use Dataframe
import matplotlib.pyplot as plt
import seaborn as sns
# are the libraries used for data visualization
from sklearn.preprocessing import LabelEncoder
# library to encode strings to numbers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import RandomOverSampler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import make_scorer
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.metrics import roc_curve, auc
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""# importing the data

"""

mldata=pd.read_csv("mydata.csv")

mldata

mldata.shape

mldata.info()

"""# data cleaning and dealing with missing values"""

mldata['Holding_Policy_Duration'].unique()


#this code displays the unique values in the attribute.

mldata['Holding_Policy_Duration'].replace({'14+':16},inplace=True)
# this code replaces all the 14+ to 15 so that it can be an integer

mldata['Holding_Policy_Duration'].fillna(0,inplace=True)
mldata['Holding_Policy_Type'].fillna(0, inplace=True)

mldata.drop('ID', axis=1, inplace=True)

# this is to drop the id as it is not important and can mess our data

categorigal_variables=['City_Code','Health Indicator']
# these are the attributes with categorigal variables

mldata['Holding_Policy_Duration']=pd.to_numeric(mldata['Holding_Policy_Duration'])
# turns the numbers into floats

mldata['Health Indicator'].fillna('X0',inplace=True)

"""# data visualization that helps with understanding my data"""

plt.figure(figsize=(10, 5))
sns.countplot(x='Response', data=mldata)

plt.show()

# this shows that the data is totally not balanced
# i will apply the balancing and then print the barchart again

plt.figure(figsize=(8, 5))
sns.countplot(x='Accomodation_Type', data=mldata)
plt.title('Distribution of Accommodation Type')
plt.show()

numeric_columns = ['Upper_Age', 'Lower_Age', 'Reco_Policy_Premium']
sns.pairplot(mldata, hue='Response', vars=numeric_columns)
plt.suptitle('Pairplot of Numeric Columns by Response', y=1.02)
plt.show()
# this plot shows the relationship between these 3 numeric columns with the response

correlation = mldata.corr().round(2)
plt.figure(figsize = (15,8))
sns.heatmap(correlation, annot = True, cmap = 'coolwarm')

mldata.drop('Upper_Age', axis=1, inplace=True)
mldata.drop('Reco_Insurance_Type', axis=1, inplace=True)

# i removed the upper age because the colleration of it with lower age is so high

correlation = mldata.corr().round(2)
plt.figure(figsize = (14,7))
sns.heatmap(correlation, annot = True, cmap = 'YlOrBr')

"""# encoding the text into numbers"""

# these are the columns we need to encode
columns_to_label_encode = [0, 2, 3, 4,5, 6, 7]

label_encoder = LabelEncoder()

for column_index in columns_to_label_encode:
    if mldata.iloc[:, column_index].dtype == 'O':
        mldata.iloc[:, column_index] = label_encoder.fit_transform(mldata.iloc[:, column_index])

mldata

"""# splitting the data into x and y"""

X = mldata.iloc[:, :-1].values
y = mldata.iloc[:, -1].values

print(X)

print(y)

"""# splitting the data into train and test dataset"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)

X_train

y_train

X_test

y_test

"""# feature scalling"""

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""# balancing the data"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

"""print the bar chart again to make sure the data is balanced"""

y_train_resampled_series = pd.Series(y_train_resampled)
#bar chart plotting
plt.figure(figsize=(8, 6))
y_train_resampled_series.value_counts().plot(kind='bar', color=['black', 'pink'])
plt.title('Bar Chart of Resampled response Variable')
plt.xlabel('Response')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.show()

print(X_train_resampled)

print(y_train_resampled)

"""# logistic reggressionn model"""

lr_model= LogisticRegression(random_state=0)
lr_model.fit(X_train,y_train)

"""hyperparameters (using grid search)"""

param_grid = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

grid_search = GridSearchCV(lr_model, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

# the best hyperparameters
print("Best Hyperparameters:", grid_search.best_params_)

"""cross validation"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


scorer = make_scorer(accuracy_score)


cross_val_results = cross_val_score(lr_model, X, y, cv=cv, scoring=scorer)

# print out  the cross-validation results
print("Cross-Validation Results:", cross_val_results)
print("Mean Accuracy:", cross_val_results.mean())

y_pred=lr_model.predict(X_test)
print(accuracy_score(y_test, y_pred))
print(np.concatenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))

print(classification_report(y_test, y_pred))

con_2=confusion_matrix(y_test,y_pred)
#a heatmap for it to be easier to read
plt.figure(figsize=(8, 6))
sns.heatmap(con_2, annot=True, fmt='d', cmap='Reds', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

#  roc curve with the auc
roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc(fpr, tpr)))
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# random forest model"""

rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train_resampled, y_train_resampled)

"""hyperparameters"""

param_grid_rf = {
    'n_estimators': [50, 100, 150],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]}
grid_search_rf = GridSearchCV(rf_classifier, param_grid_rf, cv=5, scoring='accuracy')
grid_search_rf.fit(X_train_resampled, y_train_resampled)

print("Best Hyperparameters:", grid_search_rf.best_params_)

"""cross validation"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


scorer = make_scorer(accuracy_score)


cross_val_results = cross_val_score(rf_classifier,X_train_resampled , y_train_resampled, cv=cv, scoring=scorer)

print("Cross-Validation Results:", cross_val_results)
print("Mean Accuracy:", cross_val_results.mean())

y_pred = rf_classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy:' ,accuracy_score(y_test, y_pred))

print('Classification Report:')
print(classification_report(y_test, y_pred))

con_1=confusion_matrix(y_test,y_pred)
#a heatmap for it to be easier
plt.figure(figsize=(8, 6))
sns.heatmap(con_1, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc(fpr, tpr)))
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# SVM MODEL

"""

svm_classifier = SVC(kernel='linear', random_state=0)
svm_classifier.fit(X_train, y_train)
y_pred = svm_classifier.predict(X_test)

"""*cross* validation"""

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


scorer = make_scorer(accuracy_score)


cross_val_results = cross_val_score(svm_classifier,X_train , y_train, cv=cv, scoring=scorer)

print("Cross-Validation Results:", cross_val_results)
print("Mean Accuracy:", cross_val_results.mean())

accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy:' ,accuracy_score(y_test, y_pred))

print('Classification Report:')
print(classification_report(y_test, y_pred))

con=confusion_matrix(y_test,y_pred)
#a heatmap for it to be easier
plt.figure(figsize=(8, 6))
sns.heatmap(con, annot=True, fmt='d', cmap='Reds', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred)


roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc(fpr, tpr)))
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# nural network"""

nural_model = Sequential()
nural_model.add(Dense(64, activation='relu', input_dim=X_train.shape[1]))  # Input layer with 64 neurons
nural_model.add(Dense(32, activation='relu'))  # Hidden layer with 32 neurons
nural_model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification

# Compile the model
nural_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
nural_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2)

# Evaluate the model
y_pred = (nural_model.predict(X_test) > 0.5).astype(int)
accuracy = accuracy_score(y_test, y_pred)
accuracy_score(y_test, y_pred)

"""cross validation"""

print('Classification Report:')
print(classification_report(y_test, y_pred))

con=confusion_matrix(y_test,y_pred)
#a heatmap for it to be easier
plt.figure(figsize=(8, 6))
sns.heatmap(con, annot=True, fmt='d', cmap='Reds', cbar=False)
plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

fpr, tpr, thresholds = roc_curve(y_test, y_pred)


roc_auc = auc(fpr, tpr)

plt.plot(fpr, tpr, label='ROC curve (AUC = {:.2f})'.format(auc(fpr, tpr)))
plt.plot([0, 1], [0, 1], linestyle='--', color='blue', label='Random')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

"""# models accuracies"""

#  a list to store the accuracies
accuracies = []

# Lr model
lr_accuracy = accuracy_score(y_test, lr_model.predict(X_test))
accuracies.append(lr_accuracy)

# Rf model
rf_accuracy = accuracy_score(y_test, rf_classifier.predict(X_test))
accuracies.append(rf_accuracy)

# Support vector
svm_accuracy = accuracy_score(y_test, svm_classifier.predict(X_test))
accuracies.append(svm_accuracy)

#  nn model
nn_accuracy = accuracy_score(y_test, (nural_model.predict(X_test) > 0.5).astype(int))
accuracies.append(nn_accuracy)
# this for loop is to print the accuracies above every model because they are close accuracies
for i, accuracy in enumerate(accuracies):
    plt.text(i, accuracy + 0.01, f'{accuracy:.2f}', ha='center', va='center', color='black', fontweight='bold')
# Create a bar chart
models = ['Logistic Regression', 'Random Forest', 'support vector', 'Neural Network']
plt.bar(models, accuracies, color=['pink', 'blue', 'black', 'red'])
plt.xlabel('ML models')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Models')
plt.ylim([0, 1])
plt.show()